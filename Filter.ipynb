{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching between threshold / binarization algorithms with a certain expected efficiency for images with such noise level, considering that it is noticeable that some basic known method would not be so successful for the challenge I found an algorithm known for: Binarization Algorithm by Su et al.(http://doi.acm.org/10.1145/1815330.1815351) and have code on: https://gist.github.com/pebbie/2c17620e60c662950b02c4949b3010f2#file-su-py.\n",
    "# The method is able to filter the background by estimating the contrast from the local maximum and mininos. The algorithm was applied to images of historical documents, which indicates it as an excellent solution candidate. The algorithm has been adapted for use here.\n",
    "\n",
    "\n",
    "nfns = [\n",
    "        lambda x: np.roll(x, -1, axis=0),\n",
    "        lambda x: np.roll(np.roll(x, 1, axis=1), -1, axis=0),\n",
    "        lambda x: np.roll(x, 1, axis=1),\n",
    "        lambda x: np.roll(np.roll(x, 1, axis=1), 1, axis=0),\n",
    "        lambda x: np.roll(x, 1, axis=0),\n",
    "        lambda x: np.roll(np.roll(x, -1, axis=1), 1, axis=0),\n",
    "        lambda x: np.roll(x, -1, axis=1),\n",
    "        lambda x: np.roll(np.roll(x, -1, axis=1), -1, axis=0)\n",
    "        ]\n",
    "\n",
    "def localminmax(img, fns):\n",
    "    mi = img.astype(np.float64)\n",
    "    ma = img.astype(np.float64)\n",
    "    for i in range(len(fns)):\n",
    "        rolled = fns[i](img)\n",
    "        mi = np.minimum(mi, rolled)\n",
    "        ma = np.maximum(ma, rolled)\n",
    "    result = (ma-mi)/(mi+ma+1e-16)\n",
    "    return result\n",
    "\n",
    "def numnb(bi, fns):\n",
    "    nb = bi.astype(np.float64)\n",
    "    i = np.zeros(bi.shape, nb.dtype)\n",
    "    i[bi==bi.max()] = 1\n",
    "    i[bi==bi.min()] = 0\n",
    "    for fn in fns:\n",
    "        nb += fn(i)\n",
    "    return nb\n",
    "\n",
    "def rescale(r,maxvalue=255):\n",
    "    mi = r.min()\n",
    "    return maxvalue*(r-mi)/(r.max()-mi)\n",
    "\n",
    "def binarize_Su_et_al(img):\n",
    "    gfn = nfns\n",
    "    N_MIN = 4\n",
    "\n",
    "    \n",
    "    g = img\n",
    "    I = g.astype(np.float64)\n",
    "\n",
    "\n",
    "    cimg = localminmax(I, gfn)\n",
    "    _, ocimg = cv2.threshold(rescale(cimg).astype(g.dtype), 0, 1, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    E = ocimg.astype(np.float64)\n",
    "\n",
    "\n",
    "    N_e = numnb(ocimg, gfn)\n",
    "    nbmask = N_e>0\n",
    "\n",
    "    E_mean = np.zeros(I.shape, dtype=np.float64)\n",
    "    for fn in gfn:\n",
    "        E_mean += fn(I)*fn(E)\n",
    "\n",
    "    E_mean[nbmask] /= N_e[nbmask]\n",
    "\n",
    "    E_var = np.zeros(I.shape, dtype=np.float64)\n",
    "    for fn in gfn:\n",
    "        tmp = (fn(I)-E_mean)*fn(E)\n",
    "        E_var += tmp*tmp\n",
    "\n",
    "    E_var[nbmask] /= N_e[nbmask]\n",
    "    E_std = np.sqrt(E_var)*.5\n",
    "\n",
    "    R = np.ones(I.shape)*255\n",
    "    R[(I<=E_mean+E_std)&(N_e>=N_MIN)] = 0\n",
    "\n",
    "    return R.astype(np.uint8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The solution took place in three stages.\n",
    "# The first, responsible for removing the background from the image (Algorithm by Su et al) and two others responsible for performing transformations for alignment.\n",
    "# The first alignment step was the horizontal alignment step. The idea of alignment came from the detection of the line by the Hough probabilistic transform. This transform receives the image already transformed by three other filters: a Gaussian filter, a sharpen filter to highlight edges and then a canny filter is applied.\n",
    "# With the detection of the lines it is possible to use one of them and check the horizontal alignment between the two vertices of the line. For alignment, warpAffine is then applied, which rotates 1 degree in the image clockwise or counter clockwise until the two vertices align on the y axis, more specifically, if vertex A is less than B the algorithm rotates the image in the direction clockwise and vice versa until the two align.\n",
    "# For the second alignment step, the lines detected with this same process also have a good use, in this case, after the rigid transformation done by warpAffine we now perform a non-rigid transformation on the image through warpPerspective. The idea is, given two lines aligned in y, detected by the hough algorithm, it is believed that for a good alignment, the coordinates of the two ends should be aligned in x, with tests it was verified that a possible, but not optimal, value of be used to be 5 pixels of movement for each pixel of difference between the coordinates of the two lines. An example of execution: If the top line has a pixel in front of the bottom line, then the algorithm will shift the top corners 5 pixels to the left and the bottom corners 5 pixels to the right in the final image and vice versa up to the two lines line up.\n",
    "# The downside of these transformation steps is that due to inaccurate line detection in some cases, the estimate is affected, making it impossible to achieve a better aligned result.\n",
    "\n",
    "\n",
    "\n",
    "#Edge enhance\n",
    "def sharpen_img(img):\n",
    "    kernel = np.array([[-1,-1,-1,-1,-1],\n",
    "                    [-1,2,2,2,-1],\n",
    "                    [-1,2,8,2,-1],\n",
    "                    [-2,2,2,2,-1],\n",
    "                    [-1,-1,-1,-1,-1]])/8.0\n",
    "    result=cv2.filter2D(img,-1,kernel)\n",
    "    return result\n",
    "\n",
    "def euclidian_distance(l):\n",
    "  result = math.sqrt((l[2] - l[0])**2+(l[3] - l[1])**2)\n",
    "  return result\n",
    "\n",
    "def execute_rotation(img,angle):\n",
    "  h, w = img.shape\n",
    "  center = w // 2, h // 2\n",
    "  matrix = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "  rotated = cv2.warpAffine(img, matrix, (w, h), flags=cv2.INTER_NEAREST, borderValue=255)\n",
    "  return rotated\n",
    "\n",
    "def execute_transformation(img,aux,top_steps,bottom_steps):\n",
    "  #Taking the height and width of source and target image\n",
    "  height_src_img,width_src_img = img.shape\n",
    "  height_tgt_img,width_tgt_img = aux.shape\n",
    "\n",
    "  #Taking the corners of the images: top-left, bottom-left, bottom-right, top-right\n",
    "  array_corners_source_img = np.float32([[0,0],[0,height_src_img],[width_src_img,height_src_img],[width_src_img,0]])\n",
    "  array_corners_target_img = np.float32([[top_steps,0],[0+bottom_steps,height_tgt_img],[width_tgt_img+bottom_steps,height_tgt_img],[width_tgt_img+top_steps,0]])\n",
    "     \n",
    "  # Apply Perspective Transform Algorithm \n",
    "  matrix = cv2.getPerspectiveTransform(array_corners_source_img, array_corners_target_img) \n",
    "  result = cv2.warpPerspective(img, matrix, (width_tgt_img,height_tgt_img),flags=cv2.INTER_NEAREST, borderValue=255)\n",
    "  return result\n",
    "\n",
    "def evaluate_warpingAffine(img):\n",
    "  ksize = (10, 10) \n",
    "\n",
    "  final_angle = 0\n",
    "  angle = 0\n",
    "  execute = True\n",
    "\n",
    "  aux = cv2.blur(img, ksize)\n",
    "  aux = sharpen_img(aux)\n",
    "\n",
    "\n",
    "  edges = cv2.Canny(aux, 50, 200, None, 3)\n",
    "  hough_lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 50, None, 50, 10)\n",
    "\n",
    "\n",
    "  hough_distances = []\n",
    "\n",
    "  if hough_lines is not None:\n",
    "      for i in range(0, len(hough_lines)):\n",
    "          hough_distances.append((euclidian_distance(hough_lines[i][0]),i))\n",
    "\n",
    "\n",
    "  hough_distances.sort()\n",
    "  hough_distances.reverse()\n",
    "\n",
    "  l = hough_lines[hough_distances[0][1]][0]\n",
    "\n",
    "  while(execute):\n",
    "\n",
    "    hough_distances = []\n",
    "    #Verify if aligned on horizontal\n",
    "    if l[1] > l[3]:\n",
    "      angle = -1\n",
    "      final_angle += angle\n",
    "    elif l[1] < l[3]:\n",
    "      angle = 1\n",
    "      final_angle += angle\n",
    "    else:\n",
    "      execute = False\n",
    "    edges = execute_rotation(edges,angle)\n",
    "    hough_lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 50, None, 50, 10)\n",
    "    if hough_lines is not None:\n",
    "      for i in range(0, len(hough_lines)):\n",
    "          hough_distances.append((euclidian_distance(hough_lines[i][0]),i))\n",
    "    hough_distances.sort()\n",
    "    hough_distances.reverse()\n",
    "    l = hough_lines[hough_distances[0][1]][0]\n",
    "\n",
    "  return execute_rotation(img,final_angle),hough_distances,hough_lines,edges\n",
    "\n",
    "def evaluate_warpingPerspective(img,hough_distances,hough_lines,edges):\n",
    "   \n",
    "\n",
    "  final_top_steps = 0\n",
    "  final_bottom_steps = 0\n",
    "  \n",
    "  top_steps = 0\n",
    "  bottom_steps = 0\n",
    "  execute = True\n",
    "  aux = np.zeros((edges.shape[0]+80,edges.shape[1]+80), np.uint8)\n",
    "\n",
    "  l = hough_lines[hough_distances[0][1]][0]\n",
    "  l1 = hough_lines[hough_distances[1][1]][0]\n",
    "\n",
    "  while(execute):\n",
    "    hough_distances = []\n",
    "    #Verify if aligned on Vertical and horizontal\n",
    "    #also, condition execute transformation just if distance in from A to B in x is greater than 3 \n",
    "    if (l[0] > l1[0]) and ((l[0] - l1[0]) > 3) and (l[1] < l[3]):\n",
    "      top_steps  += 5\n",
    "      bottom_steps -= 5\n",
    "      final_top_steps = top_steps\n",
    "      final_bottom_steps = bottom_steps\n",
    "    elif (l[0] < l1[0]) and  ((l1[0] - l[0]) > 3) and (l[1] > l[3]):\n",
    "      top_steps -= 5\n",
    "      bottom_steps += 5\n",
    "      final_top_steps = top_steps\n",
    "      final_bottom_steps = bottom_steps\n",
    "    else:\n",
    "      execute = False\n",
    "    \n",
    "    edges = execute_transformation(edges,aux,top_steps,bottom_steps) \n",
    "    hough_lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 50, None, 50, 10)\n",
    "    \n",
    "    if hough_lines is not None:\n",
    "      for i in range(0, len(hough_lines)):\n",
    "          hough_distances.append((euclidian_distance(hough_lines[i][0]),i))\n",
    "    \n",
    "    hough_distances.sort()\n",
    "    hough_distances.reverse()\n",
    "    \n",
    "    l = hough_lines[hough_distances[0][1]][0]\n",
    "    l1 = hough_lines[hough_distances[1][1]][0]\n",
    "\n",
    "\n",
    "  return execute_transformation(img,aux,final_top_steps,final_bottom_steps)\n",
    "  \n",
    "def execute_ALL(image_directory):\n",
    "\n",
    "  pathImages = os.listdir(image_directory)\n",
    "  pathImages.sort()\n",
    "\n",
    "  for index in range(len(pathImages)):\n",
    "    img = cv2.imread(image_directory+\"/\"+pathImages[index])\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    aux = binarize_Su_et_al(gray)\n",
    "\n",
    "    rotated_img,hough_distances,hough_lines,edges = evaluate_warpingAffine(aux)\n",
    "    rotated_img = evaluate_warpingPerspective(rotated_img,hough_distances,hough_lines,edges)\n",
    "    \n",
    "\n",
    "\n",
    "    name, ext = os.path.splitext(pathImages[index])\n",
    "    cv2.imwrite(\"denoised_data/\"+name+\"_denoised_\"+ext, rotated_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_ALL(\"noisy_data\")"
   ]
  }
 ]
}